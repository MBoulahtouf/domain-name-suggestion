{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "# Domain Name Suggestion Generator using IsraaH Model (Chat Template Version)\n",
    "\n",
    "This notebook demonstrates how to use the specialized domain name suggestion model `IsraaH/domain-name-suggestion-generator` on Google Colab with GPU acceleration, following the chat template approach shown in the official example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9ZiY8RZfEYv"
   },
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrGg2QfCfDyL"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3S56JvCfO5S"
   },
   "source": [
    "## 2. Check GPU Availability\n",
    "\n",
    "Let's check if we have access to a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz8p43lFfN9g"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7d4b8A-fVfT"
   },
   "source": [
    "## 3. Load the Domain Name Suggestion Model\n",
    "\n",
    "We'll use the specialized `IsraaH/domain-name-suggestion-generator` model which is already fine-tuned for domain name suggestions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kz5u8hK-fU5j"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_id = \"IsraaH/domain-name-suggestion-generator\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    "-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0b3y3bGflq9"
   },
   "source": [
    "## 4. Define Domain Suggestion Function\n",
    "\n",
    "Let's create a function that uses the chat template approach to generate domain suggestions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93n44D7_fku9"
   },
   "outputs": [],
   "source": [
    "def generate_domain_suggestions(business_description, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Generate domain name suggestions for a business description using the chat template approach.\n",
    "    \"\"\"\n",
    "    # Create a conversation with the business description\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Generate {num_suggestions} domain name suggestions for: {business_description}\"}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HfPfzUHgIa9"
   },
   "source": [
    "## 5. Test the Domain Suggestion System\n",
    "\n",
    "Let's test our domain suggestion system with some example business descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ0X9jxngH54"
   },
   "outputs": [],
   "source": [
    "# Example business descriptions\n",
    "test_businesses = [\n",
    "    \"A bakery that specializes in artisanal sourdough bread and pastries\",\n",
    "    \"An online platform for learning coding and software development skills\",\n",
    "    \"A sustainable fashion brand that creates eco-friendly clothing from recycled materials\",\n",
    "    \"A mobile app for tracking fitness goals and connecting with personal trainers\"\n",
    "-]\n",
    "\n",
    "# Generate suggestions for each business\n",
    "for i, business in enumerate(test_businesses, 1):\n",
    "    print(f\"\\n{i}. Business: {business}\\n\")\n",
    "    suggestions = generate_domain_suggestions(business)\n",
    "    print(f\"Domain Suggestions:\\n{suggestions}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDlGfL3GgjW3"
   },
   "source": [
    "## 6. Alternative Approach Using Pipeline\n",
    "\n",
    "Let's also try using the pipeline approach as shown in the official example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXkHfXjEgi6g"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"IsraaH/domain-name-suggestion-generator\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    "-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz14x2nthE9o"
   },
   "outputs": [],
   "source": [
    "def generate_domain_suggestions_pipeline(business_description, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Generate domain name suggestions using the pipeline approach.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Generate {num_suggestions} domain name suggestions for: {business_description}\"}\n",
    "    ]\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    return outputs[0][\"generated_text\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH2J0y2MgzgN"
   },
   "source": [
    "## 7. Testing the Pipeline Approach\n",
    "\n",
    "Let's test our pipeline-based implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8X00F5kHgy9o"
   },
   "outputs": [],
   "source": [
    "# Test the pipeline version with one business\n",
    "print(\"Testing pipeline approach...\")\n",
    "business = test_businesses[0]\n",
    "print(f\"\\nBusiness: {business}\\n\")\n",
    "suggestions = generate_domain_suggestions_pipeline(business)\n",
    "print(f\"Domain Suggestions:\\n{suggestions}\\n\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_D7d7fDdhF0u"
   },
   "source": [
    "## 8. Batch Processing Implementation\n",
    "\n",
    "Let's implement batch processing for multiple businesses using the direct model approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk9JwFVUhQ4P"
   },
   "outputs": [],
   "source": [
    "def generate_domain_suggestions_batch(business_descriptions, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Generate domain name suggestions for multiple business descriptions using batch processing.\n",
    "    \"\"\"\n",
    "    # Create conversations for all business descriptions\n",
    "    all_messages = []\n",
    "    for business in business_descriptions:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"Generate {num_suggestions} domain name suggestions for: {business}\"}\n",
    "        ]\n",
    "        all_messages.append(messages)\n",
    "    \n",
    "    # Apply chat template to all conversations\n",
    "    all_inputs = []\n",
    "    for messages in all_messages:\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "        all_inputs.append(inputs)\n",
    "    \n",
    "    # For simplicity, we'll process each one individually in this example\n",
    "    # True batch processing would require padding and more complex handling\n",
    "    responses = []\n",
    "    for inputs in all_inputs:\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode the response\n",
    "        response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK2H2QD1hj93"
   },
   "source": [
    "## 9. Testing Batch Processing\n",
    "\n",
    "Let's test our batch processing implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz14x2nthE9o"
   },
   "outputs": [],
   "source": [
    "# Test batch processing\n",
    "print(\"Testing batch processing...\")\n",
    "batch_responses = generate_domain_suggestions_batch(test_businesses)\n",
    "\n",
    "for i, (business, response) in enumerate(zip(test_businesses, batch_responses), 1):\n",
    "    print(f\"\\n{i}. Business: {business}\\n\")\n",
    "    print(f\"Domain Suggestions:\\n{response}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH2J0y2MgzgN"
   },
   "source": [
    "## 10. Structured Output Generation\n",
    "\n",
    "Let's try to get more structured output from the model by modifying our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8X00F5kHgy9o"
   },
   "outputs": [],
   "source": [
    "def generate_structured_domain_suggestions(business_description, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Generate domain name suggestions in a more structured format.\n",
    "    \"\"\"\n",
    "    # Create a conversation with a more specific prompt for structured output\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Generate {num_suggestions} domain name suggestions for: {business_description}. Format each suggestion as 'domain.com (confidence: 0.XX)' on a new line.\"}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ0X9jxngH54"
   },
   "outputs": [],
   "source": [
    "# Test structured output\n",
    "print(\"Testing structured output...\")\n",
    "business = test_businesses[0]\n",
    "print(f\"\\nBusiness: {business}\\n\")\n",
    "suggestions = generate_structured_domain_suggestions(business)\n",
    "print(f\"Domain Suggestions:\\n{suggestions}\\n\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH2J0y2MgzgN"
   },
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "1. Run the specialized `IsraaH/domain-name-suggestion-generator` model on Google Colab with GPU acceleration\n",
    "2. Use both the direct model loading approach and the pipeline approach\n",
    "3. Generate domain name suggestions using the chat template format\n",
    "4. Implement batch processing for multiple businesses\n",
    "5. Format the output in a more structured way\n",
    "\n",
    "Key advantages of this approach:\n",
    "1. **No Fine-tuning Required**: The model is already specialized for domain name suggestions\n",
    "2. **Chat Template Compatibility**: Uses the modern chat template approach for better conversation handling\n",
    "3. **Multiple Implementation Options**: Shows both direct model loading and pipeline approaches\n",
    "4. **Batch Processing Support**: Implemented for handling multiple requests efficiently\n",
    "5. **Structured Output Options**: Demonstrates how to format the output for easier parsing\n",
    "\n",
    "To use this in production, you would want to:\n",
    "1. Implement additional post-processing to ensure domain name validity\n",
    "2. Add a database of already registered domains to filter out unavailable names\n",
    "3. Implement rate limiting and caching for better performance\n",
    "4. Add safety filters to prevent generation of inappropriate domain names"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}