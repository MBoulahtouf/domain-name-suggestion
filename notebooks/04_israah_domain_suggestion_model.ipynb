{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "# Domain Name Suggestion Generator using IsraaH Model\n",
    "\n",
    "This notebook demonstrates how to use the specialized domain name suggestion model `IsraaH/domain-name-suggestion-generator` on Google Colab with GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9ZiY8RZfEYv"
   },
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrGg2QfCfDyL"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3S56JvCfO5S"
   },
   "source": [
    "## 2. Check GPU Availability\n",
    "\n",
    "Let's check if we have access to a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz8p43lFfN9g"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7d4b8A-fVfT"
   },
   "source": [
    "## 3. Load the Domain Name Suggestion Model\n",
    "\n",
    "We'll use the specialized `IsraaH/domain-name-suggestion-generator` model which is already fine-tuned for domain name suggestions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kz5u8hK-fU5j"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model ID for the domain name suggestion generator\n",
    "model_id = \"IsraaH/domain-name-suggestion-generator\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load model with optimizations for Colab\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0b3y3bGflq9"
   },
   "source": [
    "## 4. Define Domain Suggestion Function\n",
    "\n",
    "Let's create a function that uses the specialized model to generate domain suggestions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93n44D7_fku9"
   },
   "outputs": [],
   "source": [
    "def generate_domain_suggestions(business_description, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Generate domain name suggestions for a business description using the specialized model.\n",
    "    \"\"\"\n",
    "    # Create a prompt for the model\n",
    "    prompt = f\"\"\"Generate {num_suggestions} domain name suggestions for the following business:\n",
",
    "\n",
",
    "Business: {business_description}\n",
",
    "\n",
",
    "Domain names:\"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated part (after the prompt)\n",
    "    if prompt in response:\n",
    "        response = response.split(prompt)[1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HfPfzUHgIa9"
   },
   "source": [
    "## 5. Test the Domain Suggestion System\n",
    "\n",
    "Let's test our domain suggestion system with some example business descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ0X9jxngH54"
   },
   "outputs": [],
   "source": [
    "# Example business descriptions\n",
    "test_businesses = [\n",
    "    \"A bakery that specializes in artisanal sourdough bread and pastries\",\n",
    "    \"An online platform for learning coding and software development skills\",\n",
    "    \"A sustainable fashion brand that creates eco-friendly clothing from recycled materials\",\n",
    "    \"A mobile app for tracking fitness goals and connecting with personal trainers\"\n",
    "]\n",
    "\n",
    "# Generate suggestions for each business\n",
    "for i, business in enumerate(test_businesses, 1):\n",
    "    print(f\"\\n{i}. Business: {business}\\n\")\n",
    "    suggestions = generate_domain_suggestions(business)\n",
    "    print(f\"Domain Suggestions:\\n{suggestions}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDlGfL3GgjW3"
   },
   "source": [
    "## 6. Optimized Version for Better Performance\n",
    "\n",
    "Let's create a more optimized version that formats the output in a structured way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXkHfXjEgi6g"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_structured_domain_suggestions(business_description, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Generate domain name suggestions in a structured JSON-like format.\n",
    "    \"\"\"\n",
    "    # Create a prompt that encourages structured output\n",
    "    prompt = f\"\"\"Generate {num_suggestions} domain name suggestions for the following business.\n",
",
    "Format each suggestion as \"domain.com (confidence: 0.XX)\" on a new line.\n",
",
    "\n",
",
    "Business: {business_description}\n",
",
    "\n",
",
    "Domain names:\n",
",
    "1.\"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated part\n",
    "    if prompt in response:\n",
    "        response = response.split(prompt)[1].strip()\n",
    "    \n",
    "    # Add the \"1.\" that was part of the prompt\n",
    "    response = \"1.\" + response\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH2J0y2MgzgN"
   },
   "source": [
    "## 7. Testing the Structured Version\n",
    "\n",
    "Let's test our optimized structured version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8X00F5kHgy9o"
   },
   "outputs": [],
   "source": [
    "# Test the structured version\n",
    "print(\"Testing structured domain suggestions...\")\n",
    "\n",
    "for i, business in enumerate(test_businesses, 1):\n",
    "    print(f\"\\n{i}. Business: {business}\\n\")\n",
    "    suggestions = generate_structured_domain_suggestions(business)\n",
    "    print(f\"Domain Suggestions:\\n{suggestions}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_D7d7fDdhF0u"
   },
   "source": [
    "## 8. Batch Processing for Multiple Businesses\n",
    "\n",
    "Let's implement batch processing for better performance when handling multiple businesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz14x2nthE9o"
   },
   "outputs": [],
   "source": [
    "def generate_domain_suggestions_batch(business_descriptions, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Generate domain name suggestions for multiple business descriptions using batch processing.\n",
    "    \"\"\"\n",
    "    # Create prompts for all business descriptions\n",
    "    prompts = []\n",
    "    for business in business_descriptions:\n",
    "        prompt = f\"\"\"Generate {num_suggestions} domain name suggestions for the following business:\n",
",
    "\n",
",
    "Business: {business}\n",
",
    "\n",
",
    "Domain names:\"\"\"\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # Tokenize all prompts\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Generate responses\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode responses\n",
    "    responses = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        # Extract just the generated part\n",
    "        prompt = prompts[i]\n",
    "        if prompt in response:\n",
    "            response = response.split(prompt)[1].strip()\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK2H2QD1hj93"
   },
   "source": [
    "## 9. Testing Batch Processing\n",
    "\n",
    "Let's test our batch processing implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk9JwFVUhQ4P"
   },
   "outputs": [],
   "source": [
    "# Test batch processing\n",
    "print(\"Testing batch processing...\")\n",
    "batch_responses = generate_domain_suggestions_batch(test_businesses)\n",
    "\n",
    "for i, (business, response) in enumerate(zip(test_businesses, batch_responses), 1):\n",
    "    print(f\"\\n{i}. Business: {business}\\n\")\n",
    "    print(f\"Domain Suggestions:\\n{response}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH2J0y2MgzgN"
   },
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "1. Run the specialized `IsraaH/domain-name-suggestion-generator` model on Google Colab with GPU acceleration\n",
    "2. Generate domain name suggestions using the model\n",
    "3. Format the output in a structured way\n",
    "4. Optimize performance with batch processing\n",
    "\n",
    "Advantages of using this specialized model:\n",
    "1. No need for fine-tuning as it's already specialized for domain name suggestions\n",
    "2. Better performance on the specific task compared to general-purpose models\n",
    "3. More relevant and accurate domain suggestions\n",
    "4. Faster inference as it's optimized for this specific task\n",
    "\n",
    "To use this in production, you would want to:\n",
    "1. Implement additional post-processing to ensure domain name validity\n",
    "2. Add a database of already registered domains to filter out unavailable names\n",
    "3. Implement rate limiting and caching for better performance\n",
    "4. Add safety filters to prevent generation of inappropriate domain names"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}