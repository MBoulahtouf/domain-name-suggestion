{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "# Running Mistral 7B on Google Colab with GPU\n",
    "\n",
    "This notebook demonstrates how to run the Mistral 7B model using Google Colab's GPU capabilities for the domain suggestion task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9ZiY8RZfEYv"
   },
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's install the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrGg2QfCfDyL"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3S56JvCfO5S"
   },
   "source": [
    "## 2. Check GPU Availability\n",
    "\n",
    "Let's check if we have access to a GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz8p43lFfN9g"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7d4b8A-fVfT"
   },
   "source": [
    "## 3. Load the Mistral 7B Model\n",
    "\n",
    "We'll use the Mistral-7B-Instruct-v0.2 model which is well-suited for instruction following tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kz5u8hK-fU5j"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model ID for Mistral 7B Instruct v0.2\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load model with optimizations for Colab\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0b3y3bGflq9"
   },
   "source": [
    "## 4. Define Domain Suggestion Function\n",
    "\n",
    "Let's create a function that uses the Mistral model to generate domain suggestions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93n44D7_fku9"
   },
   "outputs": [],
   "source": [
    "def generate_domain_suggestions(business_description, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Generate domain name suggestions for a business description using Mistral 7B.\n",
    "    \"\"\"\n",
    "    # Create a prompt for the model\n",
    "    prompt = f\"\"\"[INST] You are a domain name suggestion expert. Based on the business description, suggest {num_suggestions} domain names with different extensions (.com, .net, .io, .co, .ai, etc.).\n",
    "\n",
    "Business Description: {business_description}\n",
    "\n",
    "Provide your suggestions in the following format:\n",
    "1. domain1.com (confidence: 0.95)\n",
    "2. domain2.net (confidence: 0.87)\n",
    "...\n",
    "[/INST]\"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the assistant's response\n",
    "    start_marker = \"[/INST]\"\n",
    "    if start_marker in response:\n",
    "        response = response.split(start_marker)[1].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HfPfzUHgIa9"
   },
   "source": [
    "## 5. Test the Domain Suggestion System\n",
    "\n",
    "Let's test our domain suggestion system with some example business descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZ0X9jxngH54"
   },
   "outputs": [],
   "source": [
    "# Example business descriptions\n",
    "test_businesses = [\n",
    "    \"A bakery that specializes in artisanal sourdough bread and pastries\",\n",
    "    \"An online platform for learning coding and software development skills\",\n",
    "    \"A sustainable fashion brand that creates eco-friendly clothing from recycled materials\",\n",
    "    \"A mobile app for tracking fitness goals and connecting with personal trainers\"\n",
    "]\n",
    "\n",
    "# Generate suggestions for each business\n",
    "for i, business in enumerate(test_businesses, 1):\n",
    "    print(f\"\\n{i}. Business: {business}\\n\")\n",
    "    suggestions = generate_domain_suggestions(business)\n",
    "    print(f\"Domain Suggestions:\\n{suggestions}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDlGfL3GgjW3"
   },
   "source": [
    "## 6. Optimized Version for Better Performance\n",
    "\n",
    "Let's create a more optimized version that uses batching for better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXkHfXjEgi6g"
   },
   "outputs": [],
   "source": [
    "def generate_domain_suggestions_batch(business_descriptions, num_suggestions=5):\n",
    "    \"\"\"\n",
    "    Generate domain name suggestions for multiple business descriptions using batch processing.\n",
    "    \"\"\"\n",
    "    # Create prompts for all business descriptions\n",
    "    prompts = []\n",
    "    for business in business_descriptions:\n",
    "        prompt = f\"\"\"[INST] You are a domain name suggestion expert. Based on the business description, suggest {num_suggestions} domain names with different extensions (.com, .net, .io, .co, .ai, etc.).\n",
    "\n",
    "Business Description: {business}\n",
    "\n",
    "Provide your suggestions in the following format:\n",
    "1. domain1.com (confidence: 0.95)\n",
    "2. domain2.net (confidence: 0.87)\n",
    "...\n",
    "[/INST]\"\"\"\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    # Tokenize all prompts\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "    # Generate responses\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=500,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    # Decode responses\n",
    "    responses = []\n",
    "    for output in outputs:\n",
    "        response = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        # Extract just the assistant's response\n",
    "        start_marker = \"[/INST]\"\n",
    "        if start_marker in response:\n",
    "            response = response.split(start_marker)[1].strip()\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH2J0y2MgzgN"
   },
   "source": [
    "## 7. Testing the Batch Version\n",
    "\n",
    "Let's test our optimized batch version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8X00F5kHgy9o"
   },
   "outputs": [],
   "source": [
    "# Test the batch version\n",
    "print(\"Testing batch processing...\")\n",
    "batch_responses = generate_domain_suggestions_batch(test_businesses)\n",
    "\n",
    "for i, (business, response) in enumerate(zip(test_businesses, batch_responses), 1):\n",
    "    print(f\"\\n{i}. Business: {business}\\n\")\n",
    "    print(f\"Domain Suggestions:\\n{response}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_D7d7fDdhF0u"
   },
   "source": [
    "## 8. Fine-tuning for Domain Suggestion Task\n",
    "\n",
    "If you want to fine-tune the model specifically for domain suggestion, here's a basic approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz14x2nthE9o"
   },
   "outputs": [],
   "source": [
    "# Install additional libraries for fine-tuning\n",
    "!pip install -q peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk9JwFVZhQ4P"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Configuration for LoRA fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK2H2QD1hj93"
   },
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "1. Run Mistral 7B on Google Colab with GPU acceleration\n",
    "2. Generate domain name suggestions using the model\n",
    "3. Optimize performance with batch processing\n",
    "4. Prepare for fine-tuning the model for the specific domain suggestion task\n",
    "\n",
    "To use this in production, you would want to:\n",
    "1. Fine-tune the model on your specific domain suggestion dataset\n",
    "2. Implement additional post-processing to ensure domain name validity\n",
    "3. Add a database of already registered domains to filter out unavailable names\n",
    "4. Implement rate limiting and caching for better performance"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}