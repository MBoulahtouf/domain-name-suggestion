{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Experiment Tracking Demo\n",
    "\n",
    "This notebook demonstrates how to use MLflow for experiment tracking in the domain suggestion project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"file:///tmp/mlflow-tracking\")\n",
    "\n",
    "print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List experiments\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "experiments = mlflow.list_experiments()\n",
    "print(\"Available experiments:\")\n",
    "for exp in experiments:\n",
    "    print(f\"- {exp.name} (ID: {exp.experiment_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation data\n",
    "eval_file = '../evaluation/enhanced_comprehensive_evaluation.json'\n",
    "if os.path.exists(eval_file):\n",
    "    with open(eval_file, 'r') as f:\n",
    "        eval_data = json.load(f)\n",
    "    print(\"Loaded evaluation data successfully!\")\n",
    "    print(\"Keys in evaluation data:\", list(eval_data.keys()))\n",
    "else:\n",
    "    print(f\"Evaluation data not found at {eval_file}\")\n",
    "    # Load the original evaluation data as fallback\n",
    "    original_eval_file = '../evaluation/comprehensive_evaluation.json'\n",
    "    if os.path.exists(original_eval_file):\n",
    "        with open(original_eval_file, 'r') as f:\n",
    "            eval_data = json.load(f)\n",
    "        print(\"Loaded original evaluation data successfully!\")\n",
    "    else:\n",
    "        eval_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key metrics\n",
    "if eval_data:\n",
    "    print(\"\\nKey Metrics:\")\n",
    "    print(f\"Average Confidence: {eval_data.get('average_confidence', 0):.3f}\")\n",
    "    print(f\"Domain Diversity: {eval_data.get('domain_diversity', 0):.3f}\")\n",
    "    print(f\"Average Domain Length: {eval_data.get('average_domain_length', 0):.2f}\")\n",
    "    print(f\"Total Suggestions: {eval_data.get('total_suggestions', 0)}\")\n",
    "    print(f\"Unique Domains: {eval_data.get('unique_domains', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize extension distribution\n",
    "if 'extension_distribution' in eval_data:\n",
    "    extensions = eval_data['extension_distribution']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    extensions_df = pd.DataFrame(list(extensions.items()), columns=['Extension', 'Count'])\n",
    "    sns.barplot(data=extensions_df, x='Extension', y='Count', palette='viridis')\n",
    "    plt.title('Domain Extension Distribution')\n",
    "    plt.xlabel('Extension')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence distribution\n",
    "if 'confidence_distribution' in eval_data:\n",
    "    confidence = eval_data['confidence_distribution']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    confidence_df = pd.DataFrame(list(confidence.items()), columns=['Confidence Range', 'Count'])\n",
    "    sns.barplot(data=confidence_df, x='Confidence Range', y='Count', palette='plasma')\n",
    "    plt.title('Confidence Score Distribution')\n",
    "    plt.xlabel('Confidence Range')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new MLflow run for this notebook\n",
    "mlflow.set_experiment(\"domain_suggestion_notebook_analysis\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"notebook_analysis_run\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"analysis_type\": \"notebook_demo\",\n",
    "        \"dataset_size\": eval_data.get('total_suggestions', 0)\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    if eval_data:\n",
    "        metrics = {\n",
    "            \"avg_confidence\": eval_data.get('average_confidence', 0),\n",
    "            \"domain_diversity\": eval_data.get('domain_diversity', 0),\n",
    "            \"avg_domain_length\": eval_data.get('average_domain_length', 0)\n",
    "        }\n",
    "        mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log this notebook as an artifact\n",
    "    mlflow.log_artifact(\"../notebooks/03_mlflow_demo.ipynb\")\n",
    "    \n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    print(f\"Logged notebook analysis to MLflow run: {run_id}\")\n",
    "\n",
    "print(\"\\nTo view MLflow UI, run in terminal:\")\n",
    "print(\"mlflow ui --backend-store-uri file:///tmp/mlflow-tracking\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}